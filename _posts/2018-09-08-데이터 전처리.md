---
layout: post
title: "단어의 분산표상과 백터화 그리고 데이터 전처리"
subtitle: "빠르게 배워보는 영어와 한글의 전처리"
author: "SeungHeon Doh"
header-img: "img/post-bg-miui6.jpg"
header-mask: 0.3
tags:
  - Preprocessing
  - 전처리
  - Gensim
---

> 본 문서는 다음과 같은 사이트를 참고하였습니다.<br>
> [Gensim](https://radimrehurek.com/gensim/tut1.html)<br>

이번시간에는 단어를 백터로 표현하는 방식에 대해서 알아보고자 합니다. 다음과 같은 예제를 준비했습니다

'''py
# documents 는 list 안에 string값으로 들어간다
from gensim import corpora
documents = ["Human machine interface for lab abc computer applications",
              "A survey of user opinion of computer system response time",
              "The EPS user interface management system",
              "System and human system engineering testing of EPS",
              "Relation of user perceived response time to error measurement",
              "The generation of random binary unordered trees",
              "The intersection graph of paths in trees",
              "Graph minors IV Widths of trees and well quasi ordering",
              "Graph minors A survey"]
'''

## 전처리 과정, tokenize

9개의 문서에는 작은 corpus들이 존재합니다. 이들을 tokenize 해봅시다. 여기서 tokenize는 형태소 단위의 단어로 쪼개어 단어의 백터를 만들어 낼 기초 재료를
만든다는 의미입니다. 이번시간에는 가장 기초적으로 문장을 띄어쓰기 단위로 쪼개고 그 다음에 불용어를 제거해 봅시다

'''py
# 일반적인 의미를 제거하고자, 불용어 셋을 제작해봅니다
stoplist = set('is an how what for a of the and to in'.split())
texts = [[word for word in documents.lower().split() 
          if word not in stoplist]
         for documents in documents]
'''

## 저 빈도의 단어 역시 제거

이번에는 저 빈도수의 단어를 제거하여, 유의미한 단어를 가지고 학습을 시켜보려고합니다. 우선 각각 단어의 빈도를 측정하기위해 collections라는 파이썬
라이브러리를 넣어줍시다.

'''py
# frequency 측정
from collections import defaultdict
frequency = defaultdict(int)
frequency
'''

그리고 다움과 같이 각각 토큰들의 frequency를 계산해 준다면 결과값을 볼 수 있습니다.
'''py
# doc-term frequency 
for text in texts:
    for token in text:
        print(frequency[token])
        frequency[token] += 1
'''

이제 빈도가 적은 토큰을 제거해 볼까요?

'''py
# doc-term frequency 중 1번 초과하여 등장한 토큰들에 대해서만, 보고싶다.
texts = [[token for token in text if frequency[token]>1]
       for text in texts]
texts
'''

이제 텍스트를 반환하면 다음과 같은 결과가 나옵니다.


'''py
[['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]
'''

좋습니다 이제 좀 쓸만한 소스들이 모였군요. 그렇다면 이러한 빈도를 과연 어떻게 정량화시킬수 있을까요? 그 방법은 바로 bag of words 입니다.
그리고 이때 우리에게 도움을 주는것이 바로 gensim의 dictionary 객체입니다

## Bag of words

문서를 벡터로 변화시킬때, 우리는 Bag_of_words라는 방식을 사용합니다. 각각의 단어는 vector의 요소 i애 대표되게 됩니다. 
<br>

vector_i = doc 안에서 i번째 단어가 등장한 횟수

## Gensim의 Dictionary 객체에서 할 수 있는것

> #### gensim.corpora.dictionary.Dictionary(documents=None, prune_at=2000000)
- Dictionarty 객체 : 
- token2id : dict 객체를 반환 (token -> tokenid)
- id2token : dict 객체를 반환 (tokenid -> token)
- dfs - dict 객체를 반환 (문서내 frequency:token id),문서가 얼마나 많은 토큰을 보유하는가?
- num_docs : input으로 들어온 doc의 갯수
- num_pos : corpus의 총 갯수 (처리된 토큰 개수)
- num_nnz : BOW matrix에서 0이 아닌 값들의 합, 전체

'''py
dictionary = corpora.Dictionary(texts)
# dictionary를 저장해둔다
dictionary.save('/tmp/deerwester.dict')
print(dictionary)
'''

## 하지만 한글 문서의 경우는?? POS 태깅의 필요성!

영어 문서의 경우는 띄어쓰기를 기반으로 형태소를 나눌 수 있었습니다. 하지만 한글의 경우에는 문장 내에서 의미를 나타내는 품사는 한정적입니다. 주로 명사, 형용사,
동사가 주된 역할을 하게 되죠. 이 떄문에 문장성분에 대한 이해가 필요합니다. 문장성분이란 한 문장을 구성하는 요소들을 문법적 기능에 따라 나눈 것을 가리킵니다.
여기서 문법적 기능이란 그 요소가 해당 문장 속에서 다른 요소와 어떤 (문법적) 관계를 가지면서 어떤 일을 하고 있는가를 나타냅니다. 예컨대 ‘주어’는 서술어가 
나타내는 동작이나 상태의 주체가 되는 말입니다. ‘목적어’는 타동사가 쓰인 문장에서 동작의 대상이 되는 말입니다. ‘서술어’는 한 문장에서 주어의 움직임, 상태, 
성질 따위를 서술하는 말입니다. 문장성분을 분석한다는 말은 문장이 주어졌을 때 주어, 목적어, 서술어 등으로 나누어 생각해본다는 뜻입니다. 문장성분을 확인할 땐 
형태론적, 통사론적, 의미론적 기준이 있습니다.

때문에 문장에서 주된 의미를 구성하는, 명사는 주로 주어와 목적어와 같은 행위나 동작의 대상, 그리고 형용사와 동사는 서술어와 같은 상태나 성질을 대변하게 됩니다.
이를 위해 저희는 각 품사에 대한 정보를 알면 좋습니다.

현재 코어닷 투데이는 API의 형태로 형태소 분석기를 제공하고 있습니다. 그 활용 방법을 보시죠

'''py
import coredottext.nlp as nlp

tclient = nlp.TextClient(api_host='unist.core.today')
tdb = tclient.lda
temp = []
def preprocessing(data, stopword):
    tdb.bin.bucket = data
    tdb.bin.tagging()
    big1 = tdb.bin.tag_to_list(pos=['NNP','NNG','VA'], limit=['term'])
    big1 = [word for word in big1 if word not in stopword]
    # 단어 길이가 1이면 제거한다.
    big2 = [token for token in big1 if len(token) > 1]
    temp.append(big2)
'''

코어닷 api는 마치 mongodb와 gemsim을 합쳐 놓은 듯한 형태를 띄고 있습니다 우선 tclient라는 객체에 tdb라는 가상의 데이터 베이스를 만들고, 그 데이터
베이스의 bin이라는 저장공간에 데이터 (문서)를 넣게 됩니다 그다음 tag_to_list라는 함수를 이용하여 내가 원하는 pos 태그(형태소 태그)와 반환의 형태
term(단어)를 얻을 수 있습니다.

새로운 한글 문서 예제를 다음과 같이 만들어 보죠

'''py
intro = '도로법 시행령 을 개정하는 데에 있어 그 개정이유와 주요내용을 국민에게 미리 알려 이에 대한 의견을 듣기 위하여 행정절차법 제조에 따라 다음과 같이 공고합니다 
년 월 일 국토교통부장관 도로법 시행령 일부개정령안 입법예고  개정이유 도로법 제조 도로점용료 감면조항에영유아보육법제조제호에 따른 어린이집 또는유아교 육법제조제호에 
따른 유치원민간어린이집과 민간유치원에 출입하기 위하여 통행로로 사용하는경우가 신설되었기에 이에 대한 하위법령을 마련하고자 함 주요내용 가 도로점용료 감면 대상 
추가안 제조제항제호가목 ㅇ 도로법 제조점용료 징수의 제한 제호 영유아보육법제조제호에 따른 어린이집 또는유아교육법제조제호에 따른 유치원에 출입하기 위하여 통행로로 
사용하는 경우 신설에 따른 하위법령 마련 나 도로점용료에 대한 경과규정 명확화안 부칙 제조 ㅇ 영 제호 도로법 시행령 일부개정령안 부칙 제조에서 규정한 도로점용료에 
대한 경과규정을 명확하게 하여 행정업무상 혼란 예방  의견제출 이 개정안에 대해 의견이 있는 기관단체 또는 개인은 년 월 일까지 통합입법예고센터 를 통하여 온라인으로 
의견을 제출하시거나 다음 사항을 기재한 의견서를 국토교통부장관에게 제출하여 주시기 바랍니다 가 예고 사항에 대한 찬성 또는 반대 의견반대 시 이유 명시 나 성명기관단체의 
경우 기관단체명과 대표자명 주소 및 전화번호다 그 밖의 참고 사항 등 제출의견 보내실 곳  일반우편   세종특별자치시 도움로  국토교통부 도로운영과 전자우편   
전화 팩스'
'''

이 문서를 해당 함수에 넣어준다면

'''py
preprocessing(i, stopword)
'''
 
'''py
['도로법', '을, '주요', '내용', '국민_행정절차법', '제조', '국토교통부장관', '도로법', '정령_입법예고_이유', '도로법', '제조', '도로', '점용료_감면', '조항', '영유아보육법', '제조', '어린이집', '유아', '육법', '제조', '유치원', '민간', '어린이집', '민간', '유치원', '출입', '통행로', '사용', '경우', '하위_법령', '마련', '주요', '내용', '도로', '점용료_감면', '대상', '추가', '제조', '도로법', '제조', '점용료_징수', '제한', '영유아보육법', '제조', '어린이집', '유아교육법', '제조', '유치원', '출입', '통행로', '사용', '경우', '하위_법령', '점용료', '경과규정', '부칙', '제조', '도로법', '정령', '부칙', '제조', '규정', '도로', '점용료', '경과', '규정', '업무상', '혼란', '예방', '의견제출_개정안_기관단체_개인', '통합_입법예고', '센터_온라인', '사항_기재_의견서_국토교통부장관', '예고_사항_찬성_반대', '반대_이유_명시', '성명', '기관단체', '경우', '단체명_대표_자명_주소', '전화', '번호', '참고', '사항', '일반_우편', '도움', '국토교통부', '도로운영과', '전자우편', '전화_팩스']
'''

다음과 같은 반환값을 얻을 수 있습니다

이제 이런 모델을 LDA학습을 위해서 다음과 같은 형태로 바꿀 수 있습니다. Gensim 라이브러리를 이용하여 dictionary와 corpus의 형태로 나타낼 수 있습니다.

'''py
from gensim import corpora
from gensim.corpora import Dictionary

# 일단 dictionary 형태로 만들어볼까?
dictionary = Dictionary(docs)

# 이제 단어를 필터링해보자!
dictionary.filter_extremes(no_above=0.5)

# 객체를 저장하자
dictionary.save('stop&bi(min=2,threshold=50,no_above=0.5).dict')
corpora.MmCorpus.serialize('stop&bi(min=2,threshold=50,no_above=0.5).mm', corpus)
'''

이제 LDA학습을 위한 전처리가 완료되었습니다